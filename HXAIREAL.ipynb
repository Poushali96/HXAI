{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xb39Ej1eY2Vo",
        "outputId": "57ac3d8b-fad0-432d-fb24-e508c112c24e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<>:294: SyntaxWarning: invalid escape sequence '\\e'\n",
            "<>:307: SyntaxWarning: invalid escape sequence '\\e'\n",
            "<>:318: SyntaxWarning: invalid escape sequence '\\e'\n",
            "<>:320: SyntaxWarning: invalid escape sequence '\\s'\n",
            "<>:332: SyntaxWarning: invalid escape sequence '\\e'\n",
            "<>:345: SyntaxWarning: invalid escape sequence '\\s'\n",
            "<>:346: SyntaxWarning: invalid escape sequence '\\e'\n",
            "<>:350: SyntaxWarning: invalid escape sequence '\\s'\n",
            "<>:350: SyntaxWarning: invalid escape sequence '\\e'\n",
            "<>:353: SyntaxWarning: invalid escape sequence '\\e'\n",
            "<>:294: SyntaxWarning: invalid escape sequence '\\e'\n",
            "<>:307: SyntaxWarning: invalid escape sequence '\\e'\n",
            "<>:318: SyntaxWarning: invalid escape sequence '\\e'\n",
            "<>:320: SyntaxWarning: invalid escape sequence '\\s'\n",
            "<>:332: SyntaxWarning: invalid escape sequence '\\e'\n",
            "<>:345: SyntaxWarning: invalid escape sequence '\\s'\n",
            "<>:346: SyntaxWarning: invalid escape sequence '\\e'\n",
            "<>:350: SyntaxWarning: invalid escape sequence '\\s'\n",
            "<>:350: SyntaxWarning: invalid escape sequence '\\e'\n",
            "<>:353: SyntaxWarning: invalid escape sequence '\\e'\n",
            "/tmp/ipython-input-2980792594.py:294: SyntaxWarning: invalid escape sequence '\\e'\n",
            "  plt.xlabel(\"Privacy Budget ($\\epsilon$)\", fontsize=12)\n",
            "/tmp/ipython-input-2980792594.py:307: SyntaxWarning: invalid escape sequence '\\e'\n",
            "  plt.xlabel(\"Privacy Budget ($\\epsilon$)\", fontsize=12)\n",
            "/tmp/ipython-input-2980792594.py:318: SyntaxWarning: invalid escape sequence '\\e'\n",
            "  plt.plot(leakages[eps], label=f\"$\\epsilon$={eps}\")\n",
            "/tmp/ipython-input-2980792594.py:320: SyntaxWarning: invalid escape sequence '\\s'\n",
            "  plt.ylabel(\"Cumulative Leakage ($\\sum \\epsilon$)\", fontsize=12)\n",
            "/tmp/ipython-input-2980792594.py:332: SyntaxWarning: invalid escape sequence '\\e'\n",
            "  plt.xlabel(\"Privacy Budget ($\\epsilon$)\", fontsize=12)\n",
            "/tmp/ipython-input-2980792594.py:345: SyntaxWarning: invalid escape sequence '\\s'\n",
            "  'True SHAP ($\\sum S_i$)': fed_shap,\n",
            "/tmp/ipython-input-2980792594.py:346: SyntaxWarning: invalid escape sequence '\\e'\n",
            "  f'DP-SHAP ($\\epsilon$={eps_to_plot})': dp_results[eps_to_plot],\n",
            "/tmp/ipython-input-2980792594.py:350: SyntaxWarning: invalid escape sequence '\\s'\n",
            "  results[['True SHAP ($\\sum S_i$)', f'DP-SHAP ($\\epsilon$={eps_to_plot})', 'Global SHAP']].plot(\n",
            "/tmp/ipython-input-2980792594.py:350: SyntaxWarning: invalid escape sequence '\\e'\n",
            "  results[['True SHAP ($\\sum S_i$)', f'DP-SHAP ($\\epsilon$={eps_to_plot})', 'Global SHAP']].plot(\n",
            "/tmp/ipython-input-2980792594.py:353: SyntaxWarning: invalid escape sequence '\\e'\n",
            "  plt.title(f'Comparison of True vs. DP-Aggregated Feature Importance ($\\epsilon$={eps_to_plot})')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting HXAI Experiment Pipeline...\n",
            "Generating appliance data...\n",
            "Fetching Norwegian price pattern (NO1)...\n",
            "⚠️ Could not fetch NordPool API: 404 Client Error: Not Found for url: https://api.open-meteo.com/v1/nordpool?zone=NO1&hourly=price. Using synthetic price pattern.\n",
            "Training local models and computing SHAP...\n",
            "Applying DP aggregation...\n",
            "Computing baselines...\n",
            "Computing metrics...\n",
            "Generating plots...\n",
            "Saving metrics to CSV...\n",
            "\n",
            "All experiments completed successfully!\n",
            "Generated files:\n",
            "- utility_curve.png\n",
            "- rank_stability.png\n",
            "- leakage_curve.png\n",
            "- rmse_curve.png\n",
            "- feature_importance_comparison.png\n",
            "- hxai_metrics.csv\n",
            "- hxai_baselines.csv\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# ============================================================\n",
        "#  HXAI EXPERIMENT REPRODUCTION SCRIPT (FULL SINGLE SCRIPT)\n",
        "#  Generates: SHAP, DP Aggregation, Utility, Ranking Stability,\n",
        "#  Leakage Curves, RMSE, Baselines, Plots, Metrics CSV\n",
        "# ============================================================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from numpy.linalg import norm\n",
        "from scipy.stats import spearmanr\n",
        "import shap\n",
        "import requests\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "\n",
        "# Suppress warnings for cleaner output\n",
        "warnings.filterwarnings('ignore', category=FutureWarning)\n",
        "warnings.filterwarnings('ignore', category=UserWarning)\n",
        "\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# ------------------------------------------\n",
        "# GLOBAL PARAMETERS\n",
        "# ------------------------------------------\n",
        "k = 50       # households\n",
        "d = 7        # appliances (Features)\n",
        "T = 168      # hours (1 week of time series data)\n",
        "epsilons = [0.25, 0.5, 1, 2, 4, 8]\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"Starting HXAI Experiment Pipeline...\")\n",
        "\n",
        "# ------------------------------------------\n",
        "# SECTION 1 — Simulated Household Data\n",
        "# ------------------------------------------\n",
        "\n",
        "print(\"Generating appliance data...\")\n",
        "\n",
        "# Average power consumption (kW) for 7 simulated appliances\n",
        "P = np.array([3.0, 2.0, 0.8, 1.2, 1.0, 0.15, 0.2])\n",
        "\n",
        "def activation_prob(hour):\n",
        "    \"\"\"\n",
        "    Defines the probability of each appliance being active based on the hour of the day.\n",
        "    Appliance index: 0-Washer, 1-Dryer, 2-Dishwasher, 3-Oven, 4-AC, 5-Lights, 6-Electronics\n",
        "    \"\"\"\n",
        "    # Normalize hour to 0-23 for daily pattern\n",
        "    h = hour % 24\n",
        "\n",
        "    if 6 <= h <= 9:  # Morning peak\n",
        "        # High probability for Washer, Dryer, Lights, Electronics\n",
        "        return np.array([0.9, 0.7, 0.1, 0.05, 0.05, 1.0, 0.8])\n",
        "    elif 17 <= h <= 21:  # Evening peak\n",
        "        # High probability for Washer, Dishwasher, Oven, AC, Lights, Electronics\n",
        "        return np.array([0.8, 0.4, 0.4, 0.3, 0.6, 1.0, 0.9])\n",
        "    else:  # Off-peak\n",
        "        return np.array([0.5, 0.3, 0.05, 0.1, 0.1, 1.0, 0.5])\n",
        "\n",
        "households = []\n",
        "\n",
        "for i in range(k):\n",
        "    # X: (T, d) matrix of hourly appliance consumption (Features)\n",
        "    # y: (T,) vector of total household consumption (Target)\n",
        "    X = np.zeros((T, d))\n",
        "    for t in range(T):\n",
        "        probs = activation_prob(t)\n",
        "        # Randomly decide if appliance is active (1) or inactive (0)\n",
        "        active = np.random.binomial(1, probs)\n",
        "        # Add small normally distributed noise to consumption\n",
        "        noise = np.random.normal(0, 0.05, d)\n",
        "        X[t] = P * active + noise\n",
        "\n",
        "    y = X.sum(axis=1) # Total consumption is the sum of appliance consumption\n",
        "    households.append((X, y))\n",
        "\n",
        "\n",
        "# ------------------------------------------\n",
        "# SECTION 2 — Norwegian Price Pattern (NO1)\n",
        "# ------------------------------------------\n",
        "\n",
        "print(\"Fetching Norwegian price pattern (NO1)...\")\n",
        "\n",
        "def fetch_no1_prices():\n",
        "    \"\"\"\n",
        "    Fetches real-time hourly NordPool energy prices for the NO1 (Oslo) zone\n",
        "    via the Open-Meteo API.\n",
        "    \"\"\"\n",
        "    url = \"https://api.open-meteo.com/v1/nordpool?zone=NO1&hourly=price\"\n",
        "\n",
        "    # Define a synthetic fallback in case the API call fails\n",
        "    synthetic_price_pattern = pd.DataFrame({\n",
        "        \"price\": np.sin(np.linspace(0, 10, T))*10 + 50\n",
        "    })\n",
        "\n",
        "    try:\n",
        "        r = requests.get(url, timeout=10)\n",
        "        r.raise_for_status() # Raise an exception for bad status codes (4xx or 5xx)\n",
        "        data = r.json()\n",
        "\n",
        "        # Ensure 'hourly' and 'price' data exists\n",
        "        if \"hourly\" not in data or \"price\" not in data[\"hourly\"]:\n",
        "            print(\"⚠️ NordPool API response missing 'price' data. Using synthetic pattern.\")\n",
        "            return synthetic_price_pattern\n",
        "\n",
        "        df = pd.DataFrame({\n",
        "            \"time\": pd.to_datetime(data[\"hourly\"][\"time\"]),\n",
        "            \"price\": data[\"hourly\"][\"price\"]\n",
        "        })\n",
        "\n",
        "        # Take the last T hours of available data\n",
        "        df = df.tail(T).reset_index(drop=True)\n",
        "\n",
        "        # Fill any potential missing values (NaNs) by carrying the backward fill\n",
        "        # (using the next available price for the missing hour)\n",
        "        df[\"price\"] = df[\"price\"].fillna(method=\"bfill\")\n",
        "\n",
        "        if len(df) < T:\n",
        "             print(f\"⚠️ NordPool API returned only {len(df)} data points. Using synthetic pattern.\")\n",
        "             return synthetic_price_pattern\n",
        "\n",
        "        return df\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"⚠️ Could not fetch NordPool API: {e}. Using synthetic price pattern.\")\n",
        "        return synthetic_price_pattern\n",
        "\n",
        "price_df = fetch_no1_prices()\n",
        "price_pattern = price_df[\"price\"].values\n",
        "\n",
        "\n",
        "# ------------------------------------------\n",
        "# SECTION 3 — Local Model + SHAP\n",
        "# ------------------------------------------\n",
        "\n",
        "print(\"Training local models and computing SHAP...\")\n",
        "\n",
        "local_models = []\n",
        "local_shap_summaries = []\n",
        "\n",
        "# Appliance names for clear visualization and output\n",
        "feature_names = [f'Appliance_{i+1}' for i in range(d)]\n",
        "\n",
        "for i in range(k):\n",
        "    X, y = households[i]\n",
        "\n",
        "    # Gradient Boosting Regressor is the local energy forecasting model\n",
        "    model = GradientBoostingRegressor(random_state=42)\n",
        "    model.fit(X, y)\n",
        "\n",
        "    # SHAP Explainer for tree-based models\n",
        "    explainer = shap.TreeExplainer(model)\n",
        "    shap_values = explainer.shap_values(X)\n",
        "\n",
        "    # S_i: The sum of SHAP values over all T time steps for household i.\n",
        "    # This represents the total feature importance (summary explanation).\n",
        "    S_i = shap_values.sum(axis=0)\n",
        "    local_models.append(model)\n",
        "    local_shap_summaries.append(S_i)\n",
        "\n",
        "# The ground truth for the zonal explanation is the aggregate of all local SHAP summaries\n",
        "fed_shap = np.sum(local_shap_summaries, axis=0)\n",
        "\n",
        "\n",
        "# ------------------------------------------\n",
        "# SECTION 4 — DP Aggregation (HXAI)\n",
        "# ------------------------------------------\n",
        "\n",
        "print(\"Applying DP aggregation...\")\n",
        "\n",
        "def laplace_noise(scale, size):\n",
        "    \"\"\"Generates random noise from a Laplace distribution.\"\"\"\n",
        "    return np.random.laplace(0, scale, size)\n",
        "\n",
        "dp_results = {}\n",
        "\n",
        "for eps in epsilons:\n",
        "    dp_summaries = []\n",
        "\n",
        "    for S in local_shap_summaries:\n",
        "        # L1-sensitivity (local clipping) is set to the L-infinity norm (max absolute value)\n",
        "        # of the feature importance vector, S.\n",
        "        sensitivity = norm(S, ord=1) # Using L1 norm as a common choice for L1 sensitivity\n",
        "        # For a Laplace mechanism: scale = sensitivity / epsilon\n",
        "        scale = sensitivity / eps\n",
        "        # Add Laplace noise to the local explanation S\n",
        "        noisy = S + laplace_noise(scale, d)\n",
        "        dp_summaries.append(noisy)\n",
        "\n",
        "    # S_zonal: The differentially private aggregate explanation\n",
        "    S_zonal = np.sum(dp_summaries, axis=0)\n",
        "    dp_results[eps] = S_zonal\n",
        "\n",
        "\n",
        "# ------------------------------------------\n",
        "# SECTION 5 — Baselines (Non-DP Comparisons)\n",
        "# ------------------------------------------\n",
        "\n",
        "print(\"Computing baselines...\")\n",
        "\n",
        "# Global SHAP (Centralized Non-Private)\n",
        "X_global = np.vstack([h[0] for h in households])\n",
        "y_global = np.hstack([h[1] for h in households])\n",
        "\n",
        "global_model = GradientBoostingRegressor(random_state=42)\n",
        "global_model.fit(X_global, y_global)\n",
        "\n",
        "global_exp = shap.TreeExplainer(global_model)\n",
        "shap_global = global_exp.shap_values(X_global)\n",
        "baseline_global = shap_global.sum(axis=0) # Aggregate SHAP from a single global model\n",
        "\n",
        "# Local DP (Data-level perturbation, less effective for aggregation)\n",
        "ldp_summaries = []\n",
        "for X, y in households:\n",
        "    # Perturb the *data* before training (DP-level on input data X)\n",
        "    X_ldp = X + np.random.laplace(0, 0.5, X.shape)\n",
        "    model_ldp = GradientBoostingRegressor(random_state=42)\n",
        "    model_ldp.fit(X_ldp, y)\n",
        "    exp_ldp = shap.TreeExplainer(model_ldp)\n",
        "    vals_ldp = exp_ldp.shap_values(X_ldp)\n",
        "    ldp_summaries.append(vals_ldp.sum(axis=0))\n",
        "baseline_ldp = np.sum(ldp_summaries, axis=0) # Aggregate SHAP from perturbed-data models\n",
        "\n",
        "# DP-SGD (Simple approximation: add noise to the non-private global model SHAP)\n",
        "# This simulates the final output of a complex DP-SGD training process.\n",
        "dp_global = baseline_global + laplace_noise(1.0, d)\n",
        "\n",
        "\n",
        "# ------------------------------------------\n",
        "# SECTION 6 — Metrics\n",
        "# ------------------------------------------\n",
        "\n",
        "print(\"Computing metrics...\")\n",
        "\n",
        "def utility(true, noisy):\n",
        "    \"\"\"\n",
        "    Computes Cosine Similarity between the true (non-private) and noisy\n",
        "    (DP-aggregated) explanation vectors. A value closer to 1 is better.\n",
        "    \"\"\"\n",
        "    # Protect against division by zero if either vector has zero norm\n",
        "    true_norm = norm(true)\n",
        "    noisy_norm = norm(noisy)\n",
        "    if true_norm == 0 or noisy_norm == 0:\n",
        "        return 0.0\n",
        "\n",
        "    return np.dot(true, noisy) / (true_norm * noisy_norm)\n",
        "\n",
        "U = {eps: utility(fed_shap, dp_results[eps]) for eps in epsilons}\n",
        "\n",
        "# Ranking Stability (Spearman's rank correlation coefficient)\n",
        "# Measures how well the noisy feature ranking matches the true ranking.\n",
        "rank_stab = {}\n",
        "for eps in epsilons:\n",
        "    # spearmanr returns (correlation, p-value)\n",
        "    rho, _ = spearmanr(fed_shap, dp_results[eps])\n",
        "    rank_stab[eps] = rho if not np.isnan(rho) else 0.0 # Handle case of constant vectors\n",
        "\n",
        "def leakage_curve(eps, max_queries=20):\n",
        "    \"\"\"\n",
        "    Simulates the growth of privacy leakage (epsilon budget exhaustion)\n",
        "    under sequential queries in a DP-compliant system.\n",
        "    \"\"\"\n",
        "    leakage = []\n",
        "    total = 0\n",
        "    # Simulate an accountant capping the total accumulated epsilon\n",
        "    max_cap = 10.0\n",
        "    for _ in range(max_queries):\n",
        "        total += eps\n",
        "        leakage.append(min(total, max_cap))\n",
        "    return leakage\n",
        "\n",
        "leakages = {eps: leakage_curve(eps) for eps in epsilons}\n",
        "\n",
        "# Root Mean Squared Error (RMSE)\n",
        "# Measures the raw error magnitude between true and noisy explanations.\n",
        "rmse = {}\n",
        "for eps in epsilons:\n",
        "    rmse[eps] = np.sqrt(mean_squared_error(fed_shap, dp_results[eps]))\n",
        "\n",
        "\n",
        "# ------------------------------------------\n",
        "# SECTION 7 — Generate Plots\n",
        "# ------------------------------------------\n",
        "\n",
        "print(\"Generating plots...\")\n",
        "\n",
        "# Utility Plot\n",
        "plt.figure(figsize=(7,5))\n",
        "plt.plot(epsilons, [U[e] for e in epsilons], marker='o', color='tab:blue')\n",
        "plt.axhline(utility(fed_shap, baseline_global), color='tab:red', linestyle='--', label='Global SHAP (Non-Private)')\n",
        "plt.xscale('log', base=2)\n",
        "plt.xlabel(\"Privacy Budget ($\\epsilon$)\", fontsize=12)\n",
        "plt.ylabel(\"Utility (Cosine Similarity)\", fontsize=12)\n",
        "plt.title(\"Explanation Utility vs Privacy Budget\", fontsize=14)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"utility_curve.png\")\n",
        "plt.close()\n",
        "\n",
        "# Ranking Stability Plot\n",
        "plt.figure(figsize=(7,5))\n",
        "plt.plot(epsilons, [rank_stab[e] for e in epsilons], marker='s', color='tab:green')\n",
        "plt.axhline(spearmanr(fed_shap, baseline_global)[0], color='tab:red', linestyle='--', label='Global SHAP (Non-Private)')\n",
        "plt.xscale('log', base=2)\n",
        "plt.xlabel(\"Privacy Budget ($\\epsilon$)\", fontsize=12)\n",
        "plt.ylabel(\"Spearman Correlation ($\\u03C1$)\", fontsize=12)\n",
        "plt.title(\"SHAP Ranking Stability vs Privacy Budget\", fontsize=14)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"rank_stability.png\")\n",
        "plt.close()\n",
        "\n",
        "# Leakage Plot\n",
        "plt.figure(figsize=(7,5))\n",
        "for eps in epsilons:\n",
        "    plt.plot(leakages[eps], label=f\"$\\epsilon$={eps}\")\n",
        "plt.xlabel(\"Query Number\", fontsize=12)\n",
        "plt.ylabel(\"Cumulative Leakage ($\\sum \\epsilon$)\", fontsize=12)\n",
        "plt.title(\"Leakage Growth Under Sequential Queries\", fontsize=14)\n",
        "plt.legend(title='Epsilon per Query')\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"leakage_curve.png\")\n",
        "plt.close()\n",
        "\n",
        "# RMSE Plot\n",
        "plt.figure(figsize=(7,5))\n",
        "plt.plot(epsilons, [rmse[e] for e in epsilons], marker='d', color='tab:orange')\n",
        "plt.axhline(np.sqrt(mean_squared_error(fed_shap, baseline_global)), color='tab:red', linestyle='--', label='Global SHAP (Non-Private)')\n",
        "plt.xscale('log', base=2)\n",
        "plt.xlabel(\"Privacy Budget ($\\epsilon$)\", fontsize=12)\n",
        "plt.ylabel(\"RMSE\", fontsize=12)\n",
        "plt.title(\"Forecasting Error (RMSE) vs Privacy Budget\", fontsize=14)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"rmse_curve.png\")\n",
        "plt.close()\n",
        "\n",
        "# Display Feature Importance (for one epsilon)\n",
        "plt.figure(figsize=(8, 6))\n",
        "eps_to_plot = 1.0 # Choose a representative epsilon\n",
        "results = pd.DataFrame({\n",
        "    'Appliance': feature_names,\n",
        "    'True SHAP ($\\sum S_i$)': fed_shap,\n",
        "    f'DP-SHAP ($\\epsilon$={eps_to_plot})': dp_results[eps_to_plot],\n",
        "    'Global SHAP': baseline_global\n",
        "}).set_index('Appliance')\n",
        "\n",
        "results[['True SHAP ($\\sum S_i$)', f'DP-SHAP ($\\epsilon$={eps_to_plot})', 'Global SHAP']].plot(\n",
        "    kind='bar', figsize=(10, 6), width=0.8, color=['tab:red', 'tab:green', 'tab:blue']\n",
        ")\n",
        "plt.title(f'Comparison of True vs. DP-Aggregated Feature Importance ($\\epsilon$={eps_to_plot})')\n",
        "plt.ylabel('Aggregate SHAP Value')\n",
        "plt.xlabel('Appliance')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.legend(title='Explanation Type')\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"feature_importance_comparison.png\")\n",
        "plt.close()\n",
        "\n",
        "\n",
        "# ------------------------------------------\n",
        "# SECTION 8 — Save CSV\n",
        "# ------------------------------------------\n",
        "\n",
        "print(\"Saving metrics to CSV...\")\n",
        "\n",
        "metrics_df = pd.DataFrame({\n",
        "    \"epsilon\": epsilons,\n",
        "    \"utility\": [U[e] for e in epsilons],\n",
        "    \"rank_stab\": [rank_stab[e] for e in epsilons],\n",
        "    \"rmse\": [rmse[e] for e in epsilons]\n",
        "})\n",
        "metrics_df.to_csv(\"hxai_metrics.csv\", index=False)\n",
        "\n",
        "# Add Baselines to a separate CSV\n",
        "baselines_df = pd.DataFrame({\n",
        "    \"Appliance\": feature_names,\n",
        "    \"Fed SHAP (Ground Truth)\": fed_shap,\n",
        "    \"Global SHAP (Centralized)\": baseline_global,\n",
        "    \"LDP (Data Perturbation)\": baseline_ldp,\n",
        "    \"DP-SGD Approx\": dp_global\n",
        "})\n",
        "baselines_df.to_csv(\"hxai_baselines.csv\", index=False)\n",
        "\n",
        "\n",
        "print(\"\\nAll experiments completed successfully!\")\n",
        "print(\"Generated files:\")\n",
        "print(\"- utility_curve.png\")\n",
        "print(\"- rank_stability.png\")\n",
        "print(\"- leakage_curve.png\")\n",
        "print(\"- rmse_curve.png\")\n",
        "print(\"- feature_importance_comparison.png\")\n",
        "print(\"- hxai_metrics.csv\")\n",
        "print(\"- hxai_baselines.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XTRauIG66r2S",
        "outputId": "2e967c49-6cbb-4307-9b01-321b3733264f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 95%|=================== | 975/1024 [00:16<00:00]       "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[saved] hxai_realdata_out/results_realdata_hxai.csv\n",
            "\n",
            "Quick check (best utility per dataset/method):\n",
            "            dataset              method   epsilon  utility_cosine  rank_spearman  mean_abs_error\n",
            "     uci_appliances       HXAI_same_eps 20.000000        0.999969       0.999296        0.234703\n",
            "     uci_appliances HXAI_negotiated_eps 20.000000        0.999957       0.999140        0.286703\n",
            "uci_household_power HXAI_negotiated_eps 20.000000        0.992340       0.503640        0.206520\n",
            "uci_household_power       HXAI_same_eps 20.000000        0.991642       0.332342        0.243742\n",
            "uci_household_power     Central_DP_SHAP 20.000000        0.776633       0.029312        0.440966\n",
            "     uci_appliances     Central_DP_SHAP  1.898235        0.652329       0.437555       29.378640\n",
            "\n",
            "Done. Check output PNGs + CSV in: hxai_realdata_out\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "import os\n",
        "import math\n",
        "import zipfile\n",
        "import argparse\n",
        "import warnings\n",
        "from dataclasses import dataclass\n",
        "from typing import Tuple, List\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from scipy.stats import spearmanr\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.ensemble import HistGradientBoostingRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "import shap\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Core helpers\n",
        "# -----------------------------\n",
        "def set_seed(seed: int = 42):\n",
        "    np.random.seed(seed)\n",
        "\n",
        "def ensure_dir(p: str):\n",
        "    os.makedirs(p, exist_ok=True)\n",
        "\n",
        "def cosine_similarity(a: np.ndarray, b: np.ndarray, eps: float = 1e-12) -> float:\n",
        "    a = np.asarray(a).reshape(-1)\n",
        "    b = np.asarray(b).reshape(-1)\n",
        "    denom = (np.linalg.norm(a) * np.linalg.norm(b)) + eps\n",
        "    return float(np.dot(a, b) / denom)\n",
        "\n",
        "def laplace_noise(scale: float, size: int) -> np.ndarray:\n",
        "    return np.random.laplace(loc=0.0, scale=scale, size=size)\n",
        "\n",
        "def rankdata_desc(v: np.ndarray) -> np.ndarray:\n",
        "    order = np.argsort(-np.abs(v))\n",
        "    ranks = np.empty_like(order)\n",
        "    ranks[order] = np.arange(1, len(v) + 1)\n",
        "    return ranks\n",
        "\n",
        "def spearman_rank_corr(a: np.ndarray, b: np.ndarray) -> float:\n",
        "    ra = rankdata_desc(a)\n",
        "    rb = rankdata_desc(b)\n",
        "    rho, _ = spearmanr(ra, rb)\n",
        "    return float(rho)\n",
        "\n",
        "def download_url(url: str, out_path: str, timeout: int = 90):\n",
        "    \"\"\"\n",
        "    Minimal downloader using stdlib only (works in Colab too).\n",
        "    \"\"\"\n",
        "    import urllib.request\n",
        "    req = urllib.request.Request(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
        "    with urllib.request.urlopen(req, timeout=timeout) as r:\n",
        "        data = r.read()\n",
        "    with open(out_path, \"wb\") as f:\n",
        "        f.write(data)\n",
        "\n",
        "def safe_read_csv(path: str, **kwargs) -> pd.DataFrame:\n",
        "    return pd.read_csv(path, **kwargs)\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class DatasetSpec:\n",
        "    name: str\n",
        "    target_col: str\n",
        "    feature_cols: List[str]\n",
        "    time_col: str = None\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Fetch datasets (auto online)\n",
        "# -----------------------------\n",
        "def fetch_uci_appliances(data_dir: str) -> Tuple[pd.DataFrame, DatasetSpec]:\n",
        "    \"\"\"\n",
        "    UCI Appliances energy prediction:\n",
        "    https://archive.ics.uci.edu/ml/machine-learning-databases/00374/energydata_complete.csv\n",
        "    \"\"\"\n",
        "    ensure_dir(data_dir)\n",
        "    url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00374/energydata_complete.csv\"\n",
        "    csv_path = os.path.join(data_dir, \"energydata_complete.csv\")\n",
        "    if not os.path.exists(csv_path):\n",
        "        print(f\"[download] {url}\")\n",
        "        download_url(url, csv_path)\n",
        "\n",
        "    df = safe_read_csv(csv_path)\n",
        "    time_col = \"date\"\n",
        "    target_col = \"Appliances\"\n",
        "    df[time_col] = pd.to_datetime(df[time_col], errors=\"coerce\")\n",
        "    df = df.dropna(subset=[time_col]).sort_values(time_col).reset_index(drop=True)\n",
        "\n",
        "    feature_cols = [c for c in df.columns if c not in [time_col, target_col]]\n",
        "    spec = DatasetSpec(name=\"uci_appliances\", target_col=target_col, feature_cols=feature_cols, time_col=time_col)\n",
        "    return df, spec\n",
        "\n",
        "\n",
        "def fetch_uci_household_power(data_dir: str) -> Tuple[pd.DataFrame, DatasetSpec]:\n",
        "    \"\"\"\n",
        "    UCI Individual household electric power consumption:\n",
        "    https://archive.ics.uci.edu/ml/machine-learning-databases/00235/household_power_consumption.zip\n",
        "    \"\"\"\n",
        "    ensure_dir(data_dir)\n",
        "    url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00235/household_power_consumption.zip\"\n",
        "    zip_path = os.path.join(data_dir, \"household_power_consumption.zip\")\n",
        "    if not os.path.exists(zip_path):\n",
        "        print(f\"[download] {url}\")\n",
        "        download_url(url, zip_path)\n",
        "\n",
        "    txt_path = os.path.join(data_dir, \"household_power_consumption.txt\")\n",
        "    if not os.path.exists(txt_path):\n",
        "        with zipfile.ZipFile(zip_path, \"r\") as zf:\n",
        "            zf.extractall(data_dir)\n",
        "\n",
        "    df = safe_read_csv(\n",
        "        txt_path,\n",
        "        sep=\";\",\n",
        "        na_values=[\"?\"],\n",
        "        low_memory=False\n",
        "    )\n",
        "\n",
        "    df[\"DateTime\"] = pd.to_datetime(df[\"Date\"] + \" \" + df[\"Time\"], errors=\"coerce\", dayfirst=True)\n",
        "    df = df.dropna(subset=[\"DateTime\"]).sort_values(\"DateTime\").reset_index(drop=True)\n",
        "\n",
        "    # numeric conversion\n",
        "    for c in df.columns:\n",
        "        if c not in [\"Date\", \"Time\", \"DateTime\"]:\n",
        "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "\n",
        "    df = df.dropna().reset_index(drop=True)\n",
        "\n",
        "    target_col = \"Global_active_power\"\n",
        "    time_col = \"DateTime\"\n",
        "    feature_cols = [c for c in df.columns if c not in [target_col, time_col, \"Date\", \"Time\"]]\n",
        "    spec = DatasetSpec(name=\"uci_household_power\", target_col=target_col, feature_cols=feature_cols, time_col=time_col)\n",
        "    return df, spec\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Supervised lag task\n",
        "# -----------------------------\n",
        "def make_supervised_lagged(\n",
        "    df: pd.DataFrame,\n",
        "    spec: DatasetSpec,\n",
        "    lags: int = 6,\n",
        "    horizon: int = 1\n",
        ") -> Tuple[pd.DataFrame, pd.Series, List[str]]:\n",
        "    df = df.copy()\n",
        "    if spec.time_col is not None:\n",
        "        df = df.sort_values(spec.time_col).reset_index(drop=True)\n",
        "\n",
        "    if spec.time_col is not None:\n",
        "        t = pd.to_datetime(df[spec.time_col])\n",
        "        df[\"hour\"] = t.dt.hour\n",
        "        df[\"dayofweek\"] = t.dt.dayofweek\n",
        "        df[\"month\"] = t.dt.month\n",
        "        base_features = spec.feature_cols + [\"hour\", \"dayofweek\", \"month\"]\n",
        "    else:\n",
        "        base_features = spec.feature_cols\n",
        "\n",
        "    X = pd.DataFrame(index=df.index)\n",
        "    for col in base_features:\n",
        "        for L in range(1, lags + 1):\n",
        "            X[f\"{col}_lag{L}\"] = df[col].shift(L)\n",
        "\n",
        "    y = df[spec.target_col].shift(-horizon)\n",
        "\n",
        "    valid = X.notnull().all(axis=1) & y.notnull()\n",
        "    X = X.loc[valid].reset_index(drop=True)\n",
        "    y = y.loc[valid].reset_index(drop=True)\n",
        "    return X, y, list(X.columns)\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# HXAI pipeline\n",
        "# -----------------------------\n",
        "def split_into_households_contiguous(X: pd.DataFrame, y: pd.Series, k: int):\n",
        "    n = len(X)\n",
        "    block = n // k\n",
        "    chunks = []\n",
        "    for i in range(k):\n",
        "        a = i * block\n",
        "        b = (i + 1) * block if i < k - 1 else n\n",
        "        chunks.append((X.iloc[a:b].reset_index(drop=True), y.iloc[a:b].reset_index(drop=True)))\n",
        "    return chunks\n",
        "\n",
        "def train_tree_model(X_train: np.ndarray, y_train: np.ndarray, seed: int = 42):\n",
        "    model = HistGradientBoostingRegressor(\n",
        "        max_depth=6,\n",
        "        learning_rate=0.08,\n",
        "        max_iter=300,\n",
        "        random_state=seed\n",
        "    )\n",
        "    model.fit(X_train, y_train)\n",
        "    return model\n",
        "\n",
        "def shap_summary_vector(model, X_bg: np.ndarray, X_eval: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Robust SHAP summary computation for tree models.\n",
        "\n",
        "    - Uses interventional perturbation (safe for partial background coverage)\n",
        "    - Disables additivity check at shap_values() call\n",
        "      (required for HistGradientBoosting + interventional SHAP)\n",
        "    \"\"\"\n",
        "    explainer = shap.TreeExplainer(\n",
        "        model,\n",
        "        data=X_bg,\n",
        "        feature_perturbation=\"interventional\"\n",
        "    )\n",
        "\n",
        "    shap_vals = explainer.shap_values(\n",
        "        X_eval,\n",
        "        check_additivity=False\n",
        "    )\n",
        "\n",
        "    shap_vals = np.asarray(shap_vals)\n",
        "    return np.mean(np.abs(shap_vals), axis=0)\n",
        "\n",
        "\n",
        "\n",
        "def hxai_zonal_aggregate(local_summaries: List[np.ndarray], epsilons: List[float], sensitivity_l1: float) -> np.ndarray:\n",
        "    d = local_summaries[0].shape[0]\n",
        "    zonal = np.zeros(d, dtype=float)\n",
        "    for Si, eps in zip(local_summaries, epsilons):\n",
        "        scale = sensitivity_l1 / max(eps, 1e-12)\n",
        "        zonal += Si + laplace_noise(scale=scale, size=d)\n",
        "    return zonal\n",
        "\n",
        "def central_dp_baseline(X: np.ndarray, y: np.ndarray, sensitivity_l1: float, epsilon: float, seed: int = 42):\n",
        "    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
        "    model = train_tree_model(Xtr, ytr, seed=seed)\n",
        "    preds = model.predict(Xte)\n",
        "    metrics = {\"mse\": float(mean_squared_error(yte, preds)), \"r2\": float(r2_score(yte, preds))}\n",
        "\n",
        "    bg = Xtr[np.linspace(0, len(Xtr) - 1, num=min(512, len(Xtr))).astype(int)]\n",
        "    evalX = Xte[np.linspace(0, len(Xte) - 1, num=min(1024, len(Xte))).astype(int)]\n",
        "\n",
        "    S = shap_summary_vector(model, bg, evalX)\n",
        "    scale = sensitivity_l1 / max(epsilon, 1e-12)\n",
        "    return S + laplace_noise(scale=scale, size=S.shape[0]), metrics\n",
        "\n",
        "def negotiate_epsilons(local_summaries: List[np.ndarray], epsilon_total: float, eps_min: float, eps_max: float, temperature: float = 0.6):\n",
        "    norms = np.array([np.linalg.norm(Si) for Si in local_summaries], dtype=float)\n",
        "    norms = np.maximum(norms, 1e-12)\n",
        "    w = norms ** temperature\n",
        "    w = w / np.sum(w)\n",
        "\n",
        "    eps = epsilon_total * w\n",
        "    eps = np.clip(eps, eps_min, eps_max)\n",
        "\n",
        "    # renormalize w/ clamps\n",
        "    for _ in range(10):\n",
        "        s = float(np.sum(eps))\n",
        "        if s <= 1e-12:\n",
        "            eps[:] = epsilon_total / len(eps)\n",
        "            break\n",
        "        eps *= (epsilon_total / s)\n",
        "        eps = np.clip(eps, eps_min, eps_max)\n",
        "    return eps.tolist()\n",
        "\n",
        "def run_dataset_experiment(df: pd.DataFrame, spec: DatasetSpec, lags: int, horizon: int,\n",
        "                           k_households: int, eps_grid: List[float], sensitivity_l1: float, seed: int = 42):\n",
        "\n",
        "    set_seed(seed)\n",
        "    Xdf, y, feat_cols = make_supervised_lagged(df, spec, lags=lags, horizon=horizon)\n",
        "\n",
        "    X = Xdf.values.astype(float)\n",
        "    yv = y.values.astype(float)\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X = scaler.fit_transform(X)\n",
        "\n",
        "    chunks = split_into_households_contiguous(pd.DataFrame(X), pd.Series(yv), k=k_households)\n",
        "\n",
        "    local_summaries = []\n",
        "    local_pred_metrics = []\n",
        "    d = X.shape[1]\n",
        "\n",
        "    for i, (Xi, yi) in enumerate(chunks):\n",
        "        Xi = Xi.values.astype(float)\n",
        "        yi = yi.values.astype(float)\n",
        "        Xtr, Xte, ytr, yte = train_test_split(Xi, yi, test_size=0.2, shuffle=False)\n",
        "        model = train_tree_model(Xtr, ytr, seed=seed + i)\n",
        "\n",
        "        preds = model.predict(Xte)\n",
        "        local_pred_metrics.append((float(mean_squared_error(yte, preds)), float(r2_score(yte, preds))))\n",
        "\n",
        "        bg = Xtr[np.linspace(0, len(Xtr) - 1, num=min(256, len(Xtr))).astype(int)]\n",
        "        evalX = Xte[np.linspace(0, len(Xte) - 1, num=min(512, len(Xte))).astype(int)]\n",
        "        local_summaries.append(shap_summary_vector(model, bg, evalX))\n",
        "\n",
        "    S_true = np.sum(np.stack(local_summaries, axis=0), axis=0)\n",
        "\n",
        "    rows = []\n",
        "    for eps in eps_grid:\n",
        "        # HXAI naive\n",
        "        S_same = hxai_zonal_aggregate(local_summaries, [eps] * k_households, sensitivity_l1)\n",
        "\n",
        "        # Special scheme (negotiated eps) with same total budget\n",
        "        eps_total = k_households * eps\n",
        "        eps_list = negotiate_epsilons(\n",
        "            local_summaries=local_summaries,\n",
        "            epsilon_total=eps_total,\n",
        "            eps_min=max(0.05, 0.10 * eps),\n",
        "            eps_max=10.0 * eps,\n",
        "            temperature=0.6\n",
        "        )\n",
        "        S_neg = hxai_zonal_aggregate(local_summaries, eps_list, sensitivity_l1)\n",
        "\n",
        "        # Central DP baseline\n",
        "        S_central, central_metrics = central_dp_baseline(X, yv, sensitivity_l1, eps, seed=seed)\n",
        "\n",
        "        for method, S_hat in [\n",
        "            (\"HXAI_same_eps\", S_same),\n",
        "            (\"HXAI_negotiated_eps\", S_neg),\n",
        "            (\"Central_DP_SHAP\", S_central),\n",
        "        ]:\n",
        "            rows.append({\n",
        "                \"dataset\": spec.name,\n",
        "                \"method\": method,\n",
        "                \"epsilon\": eps,\n",
        "                \"utility_cosine\": cosine_similarity(S_true, S_hat),\n",
        "                \"rank_spearman\": spearman_rank_corr(S_true, S_hat),\n",
        "                \"mean_abs_error\": float(np.mean(np.abs(S_true - S_hat))),\n",
        "                \"d_features\": d,\n",
        "                \"k_households\": k_households,\n",
        "                \"sensitivity_l1\": sensitivity_l1,\n",
        "                \"local_avg_mse\": float(np.mean([m for m, _ in local_pred_metrics])),\n",
        "                \"local_avg_r2\": float(np.mean([r for _, r in local_pred_metrics])),\n",
        "                \"central_mse\": central_metrics.get(\"mse\", np.nan),\n",
        "                \"central_r2\": central_metrics.get(\"r2\", np.nan),\n",
        "            })\n",
        "\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "\n",
        "def plot_curves(res: pd.DataFrame, out_dir: str, dataset_name: str):\n",
        "    ensure_dir(out_dir)\n",
        "\n",
        "    def _plot(metric: str, ylabel: str, fname: str):\n",
        "        plt.figure()\n",
        "        for method in sorted(res[\"method\"].unique()):\n",
        "            rr = res[res[\"method\"] == method].sort_values(\"epsilon\")\n",
        "            plt.plot(rr[\"epsilon\"], rr[metric], marker=\"o\", label=method)\n",
        "        plt.xscale(\"log\")\n",
        "        plt.xlabel(\"epsilon (log scale)\")\n",
        "        plt.ylabel(ylabel)\n",
        "        plt.title(f\"{dataset_name}: {ylabel} vs epsilon\")\n",
        "        plt.grid(True, which=\"both\", alpha=0.3)\n",
        "        plt.legend()\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(out_dir, fname), dpi=220)\n",
        "        plt.close()\n",
        "\n",
        "    _plot(\"utility_cosine\", \"Utility (cosine similarity)\", f\"{dataset_name}_utility_vs_epsilon.png\")\n",
        "    _plot(\"rank_spearman\", \"Ranking stability (Spearman)\", f\"{dataset_name}_rank_vs_epsilon.png\")\n",
        "    _plot(\"mean_abs_error\", \"Mean abs error\", f\"{dataset_name}_mae_vs_epsilon.png\")\n",
        "\n",
        "\n",
        "def main(args):\n",
        "    set_seed(args.seed)\n",
        "    ensure_dir(args.out_dir)\n",
        "    ensure_dir(args.data_dir)\n",
        "\n",
        "    eps_grid = np.logspace(math.log10(args.eps_min), math.log10(args.eps_max), args.eps_points).tolist()\n",
        "\n",
        "    all_results = []\n",
        "\n",
        "    df1, spec1 = fetch_uci_appliances(args.data_dir)\n",
        "    res1 = run_dataset_experiment(df1, spec1, args.lags, args.horizon, args.k_households, eps_grid, args.sensitivity_l1, seed=args.seed)\n",
        "    plot_curves(res1, args.out_dir, spec1.name)\n",
        "    all_results.append(res1)\n",
        "\n",
        "    df2, spec2 = fetch_uci_household_power(args.data_dir)\n",
        "    res2 = run_dataset_experiment(df2, spec2, args.lags, args.horizon, args.k_households, eps_grid, args.sensitivity_l1, seed=args.seed)\n",
        "    plot_curves(res2, args.out_dir, spec2.name)\n",
        "    all_results.append(res2)\n",
        "\n",
        "    res_all = pd.concat(all_results, axis=0).reset_index(drop=True)\n",
        "    csv_path = os.path.join(args.out_dir, \"results_realdata_hxai.csv\")\n",
        "    res_all.to_csv(csv_path, index=False)\n",
        "    print(f\"[saved] {csv_path}\")\n",
        "\n",
        "    print(\"\\nQuick check (best utility per dataset/method):\")\n",
        "    print(\n",
        "        res_all.sort_values(\"utility_cosine\", ascending=False)\n",
        "              .groupby([\"dataset\", \"method\"], as_index=False)\n",
        "              .head(1)[[\"dataset\",\"method\",\"epsilon\",\"utility_cosine\",\"rank_spearman\",\"mean_abs_error\"]]\n",
        "              .to_string(index=False)\n",
        "    )\n",
        "\n",
        "    print(\"\\nDone. Check output PNGs + CSV in:\", args.out_dir)\n",
        "\n",
        "\n",
        "def build_argparser():\n",
        "    ap = argparse.ArgumentParser(add_help=True)\n",
        "    ap.add_argument(\"--out_dir\", type=str, default=\"hxai_realdata_out\")\n",
        "    ap.add_argument(\"--data_dir\", type=str, default=\"hxai_realdata_data\")\n",
        "    ap.add_argument(\"--k_households\", type=int, default=20)\n",
        "    ap.add_argument(\"--lags\", type=int, default=6)\n",
        "    ap.add_argument(\"--horizon\", type=int, default=1)\n",
        "    ap.add_argument(\"--seed\", type=int, default=42)\n",
        "    ap.add_argument(\"--eps_min\", type=float, default=0.1)\n",
        "    ap.add_argument(\"--eps_max\", type=float, default=20.0)\n",
        "    ap.add_argument(\"--eps_points\", type=int, default=10)\n",
        "    ap.add_argument(\"--sensitivity_l1\", type=float, default=1.0)\n",
        "    return ap\n",
        "def make_supervised_lagged(df, spec, lags=6, horizon=1):\n",
        "    df = df.copy()\n",
        "    if spec.time_col is not None:\n",
        "        df = df.sort_values(spec.time_col).reset_index(drop=True)\n",
        "\n",
        "    if spec.time_col is not None:\n",
        "        t = pd.to_datetime(df[spec.time_col])\n",
        "        base = df[spec.feature_cols].copy()\n",
        "        base[\"hour\"] = t.dt.hour\n",
        "        base[\"dayofweek\"] = t.dt.dayofweek\n",
        "        base[\"month\"] = t.dt.month\n",
        "    else:\n",
        "        base = df[spec.feature_cols].copy()\n",
        "\n",
        "    lagged_blocks = []\n",
        "    colnames = []\n",
        "\n",
        "    for L in range(1, lags + 1):\n",
        "        shifted = base.shift(L)\n",
        "        shifted.columns = [f\"{c}_lag{L}\" for c in shifted.columns]\n",
        "        lagged_blocks.append(shifted)\n",
        "        colnames.extend(shifted.columns)\n",
        "\n",
        "    X = pd.concat(lagged_blocks, axis=1)\n",
        "    y = df[spec.target_col].shift(-horizon)\n",
        "\n",
        "    valid = X.notnull().all(axis=1) & y.notnull()\n",
        "    X = X.loc[valid].reset_index(drop=True)\n",
        "    y = y.loc[valid].reset_index(drop=True)\n",
        "\n",
        "    return X, y, colnames\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = build_argparser()\n",
        "\n",
        "    # ✅ KEY FIX: ignore unknown args injected by Jupyter/Colab (e.g., \"-f kernel.json\")\n",
        "    args, _unknown = parser.parse_known_args()\n",
        "\n",
        "    main(args)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "uRYabz0fKSQo",
        "outputId": "d45fcf34-867f-4ce7-a207-6935c6067774"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[download] https://archive.ics.uci.edu/ml/machine-learning-databases/00374/energydata_complete.csv\n",
            "[download] https://archive.ics.uci.edu/ml/machine-learning-databases/00235/household_power_consumption.zip\n",
            "[download] https://archive.ics.uci.edu/ml/machine-learning-databases/00275/Bike-Sharing-Dataset.zip\n",
            "\n",
            "[dataset] uci_appliances  n=19735  d_raw=27  time=date\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 96%|=================== | 984/1024 [00:17<00:00]       "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[dataset] uci_household_power  n=2049280  d_raw=6  time=DateTime\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 96%|=================== | 980/1024 [00:11<00:00]       "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[dataset] uci_bikesharing_hourly  n=17379  d_raw=12  time=DateTime\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 96%|=================== | 985/1024 [00:16<00:00]       "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[saved] hxai_realdata_out/results_per_seed.csv\n",
            "[saved] hxai_realdata_out/results_agg_meanstd.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 94%|=================== | 965/1024 [00:16<00:00]       "
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "\"\"\"\n",
        "HXAI real-data experiments + qualitative visualization + seed-averaged confidence bands.\n",
        "\n",
        "Datasets fetched online (no keys):\n",
        "  1) UCI Appliances energy (energydata_complete.csv)\n",
        "  2) UCI Household Power consumption (household_power_consumption.zip)\n",
        "  3) UCI Bike Sharing Hourly (Bike-Sharing-Dataset.zip)\n",
        "\n",
        "Outputs:\n",
        "  out_dir/\n",
        "    results_per_seed.csv\n",
        "    results_agg_meanstd.csv\n",
        "    curves_<dataset>_{utility,rank,mae}.(png|pdf)\n",
        "    qual_<dataset>_bars_eps{...}.(png|pdf)\n",
        "    qual_<dataset>_ranktraj_<method>.(png|pdf)\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import math\n",
        "import zipfile\n",
        "import argparse\n",
        "import warnings\n",
        "from dataclasses import dataclass\n",
        "from typing import Tuple, List, Dict, Optional\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from scipy.stats import spearmanr\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.ensemble import HistGradientBoostingRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "import shap\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Warnings\n",
        "# -----------------------------\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Utilities\n",
        "# -----------------------------\n",
        "def set_seed(seed: int):\n",
        "    np.random.seed(seed)\n",
        "\n",
        "def ensure_dir(p: str):\n",
        "    os.makedirs(p, exist_ok=True)\n",
        "\n",
        "def download_url(url: str, out_path: str, timeout: int = 120):\n",
        "    import urllib.request\n",
        "    req = urllib.request.Request(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
        "    with urllib.request.urlopen(req, timeout=timeout) as r:\n",
        "        data = r.read()\n",
        "    with open(out_path, \"wb\") as f:\n",
        "        f.write(data)\n",
        "\n",
        "def cosine_similarity(a: np.ndarray, b: np.ndarray, eps: float = 1e-12) -> float:\n",
        "    a = np.asarray(a).reshape(-1)\n",
        "    b = np.asarray(b).reshape(-1)\n",
        "    denom = (np.linalg.norm(a) * np.linalg.norm(b)) + eps\n",
        "    return float(np.dot(a, b) / denom)\n",
        "\n",
        "def laplace_noise(scale: float, size: int) -> np.ndarray:\n",
        "    return np.random.laplace(loc=0.0, scale=scale, size=size)\n",
        "\n",
        "def rankdata_desc(v: np.ndarray) -> np.ndarray:\n",
        "    order = np.argsort(-np.abs(v))\n",
        "    ranks = np.empty_like(order)\n",
        "    ranks[order] = np.arange(1, len(v) + 1)\n",
        "    return ranks\n",
        "\n",
        "def spearman_rank_corr(a: np.ndarray, b: np.ndarray) -> float:\n",
        "    ra = rankdata_desc(a)\n",
        "    rb = rankdata_desc(b)\n",
        "    rho, _ = spearmanr(ra, rb)\n",
        "    return float(rho)\n",
        "\n",
        "def topk_indices(v: np.ndarray, k: int) -> np.ndarray:\n",
        "    return np.argsort(-np.abs(v))[:k]\n",
        "\n",
        "def ieee_axes(ax):\n",
        "    # IEEE/TMLR-friendly: clean axes, readable ticks, no heavy styling\n",
        "    ax.grid(True, which=\"both\", alpha=0.25)\n",
        "    ax.spines[\"top\"].set_visible(False)\n",
        "    ax.spines[\"right\"].set_visible(False)\n",
        "\n",
        "def save_fig(fig, out_path_base: str, dpi: int = 220):\n",
        "    fig.tight_layout()\n",
        "    fig.savefig(out_path_base + \".png\", dpi=dpi)\n",
        "    fig.savefig(out_path_base + \".pdf\")\n",
        "    plt.close(fig)\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Dataset specs\n",
        "# -----------------------------\n",
        "@dataclass\n",
        "class DatasetSpec:\n",
        "    name: str\n",
        "    target_col: str\n",
        "    feature_cols: List[str]\n",
        "    time_col: Optional[str] = None\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Fetch datasets\n",
        "# -----------------------------\n",
        "def fetch_uci_appliances(data_dir: str) -> Tuple[pd.DataFrame, DatasetSpec]:\n",
        "    ensure_dir(data_dir)\n",
        "    url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00374/energydata_complete.csv\"\n",
        "    csv_path = os.path.join(data_dir, \"energydata_complete.csv\")\n",
        "    if not os.path.exists(csv_path):\n",
        "        print(f\"[download] {url}\")\n",
        "        download_url(url, csv_path)\n",
        "\n",
        "    df = pd.read_csv(csv_path)\n",
        "    time_col = \"date\"\n",
        "    target_col = \"Appliances\"\n",
        "    df[time_col] = pd.to_datetime(df[time_col], errors=\"coerce\")\n",
        "    df = df.dropna(subset=[time_col]).sort_values(time_col).reset_index(drop=True)\n",
        "    feature_cols = [c for c in df.columns if c not in [time_col, target_col]]\n",
        "    return df, DatasetSpec(\"uci_appliances\", target_col, feature_cols, time_col)\n",
        "\n",
        "\n",
        "def fetch_uci_household_power(data_dir: str) -> Tuple[pd.DataFrame, DatasetSpec]:\n",
        "    ensure_dir(data_dir)\n",
        "    url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00235/household_power_consumption.zip\"\n",
        "    zip_path = os.path.join(data_dir, \"household_power_consumption.zip\")\n",
        "    if not os.path.exists(zip_path):\n",
        "        print(f\"[download] {url}\")\n",
        "        download_url(url, zip_path)\n",
        "\n",
        "    txt_path = os.path.join(data_dir, \"household_power_consumption.txt\")\n",
        "    if not os.path.exists(txt_path):\n",
        "        with zipfile.ZipFile(zip_path, \"r\") as zf:\n",
        "            zf.extractall(data_dir)\n",
        "\n",
        "    df = pd.read_csv(txt_path, sep=\";\", na_values=[\"?\"], low_memory=False)\n",
        "\n",
        "    df[\"DateTime\"] = pd.to_datetime(df[\"Date\"] + \" \" + df[\"Time\"], errors=\"coerce\", dayfirst=True)\n",
        "    df = df.dropna(subset=[\"DateTime\"]).sort_values(\"DateTime\").reset_index(drop=True)\n",
        "\n",
        "    for c in df.columns:\n",
        "        if c not in [\"Date\", \"Time\", \"DateTime\"]:\n",
        "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "\n",
        "    df = df.dropna().reset_index(drop=True)\n",
        "\n",
        "    target_col = \"Global_active_power\"\n",
        "    time_col = \"DateTime\"\n",
        "    feature_cols = [c for c in df.columns if c not in [target_col, time_col, \"Date\", \"Time\"]]\n",
        "    return df, DatasetSpec(\"uci_household_power\", target_col, feature_cols, time_col)\n",
        "\n",
        "\n",
        "def fetch_uci_bikesharing_hourly(data_dir: str) -> Tuple[pd.DataFrame, DatasetSpec]:\n",
        "    \"\"\"\n",
        "    Bike Sharing Dataset:\n",
        "    https://archive.ics.uci.edu/ml/machine-learning-databases/00275/Bike-Sharing-Dataset.zip\n",
        "    We use hour.csv and predict cnt (total rentals).\n",
        "    \"\"\"\n",
        "    ensure_dir(data_dir)\n",
        "    url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00275/Bike-Sharing-Dataset.zip\"\n",
        "    zip_path = os.path.join(data_dir, \"Bike-Sharing-Dataset.zip\")\n",
        "    if not os.path.exists(zip_path):\n",
        "        print(f\"[download] {url}\")\n",
        "        download_url(url, zip_path)\n",
        "\n",
        "    hour_csv = os.path.join(data_dir, \"hour.csv\")\n",
        "    if not os.path.exists(hour_csv):\n",
        "        with zipfile.ZipFile(zip_path, \"r\") as zf:\n",
        "            zf.extractall(data_dir)\n",
        "\n",
        "    df = pd.read_csv(hour_csv)\n",
        "    # 'dteday' is date (no hour). actual hour is 'hr'. Keep a constructed timestamp for ordering.\n",
        "    df[\"dteday\"] = pd.to_datetime(df[\"dteday\"], errors=\"coerce\")\n",
        "    df = df.dropna(subset=[\"dteday\"]).reset_index(drop=True)\n",
        "    df[\"DateTime\"] = df[\"dteday\"] + pd.to_timedelta(df[\"hr\"].astype(int), unit=\"h\")\n",
        "    df = df.sort_values(\"DateTime\").reset_index(drop=True)\n",
        "\n",
        "    target_col = \"cnt\"\n",
        "    time_col = \"DateTime\"\n",
        "    drop_cols = {\"instant\", \"dteday\", \"DateTime\", target_col, \"casual\", \"registered\"}  # avoid leakage cols\n",
        "    feature_cols = [c for c in df.columns if c not in drop_cols]\n",
        "    return df, DatasetSpec(\"uci_bikesharing_hourly\", target_col, feature_cols, time_col)\n",
        "\n",
        "\n",
        "def get_all_datasets(data_dir: str, which: str) -> List[Tuple[pd.DataFrame, DatasetSpec]]:\n",
        "    \"\"\"\n",
        "    which: 'all' or comma-separated list among:\n",
        "      uci_appliances, uci_household_power, uci_bikesharing_hourly\n",
        "    \"\"\"\n",
        "    ds_map = {\n",
        "        \"uci_appliances\": fetch_uci_appliances,\n",
        "        \"uci_household_power\": fetch_uci_household_power,\n",
        "        \"uci_bikesharing_hourly\": fetch_uci_bikesharing_hourly,\n",
        "    }\n",
        "    if which.strip().lower() == \"all\":\n",
        "        keys = list(ds_map.keys())\n",
        "    else:\n",
        "        keys = [k.strip() for k in which.split(\",\") if k.strip()]\n",
        "        for k in keys:\n",
        "            if k not in ds_map:\n",
        "                raise ValueError(f\"Unknown dataset '{k}'. Allowed: {list(ds_map.keys())} or 'all'.\")\n",
        "\n",
        "    out = []\n",
        "    for k in keys:\n",
        "        df, spec = ds_map[k](data_dir)\n",
        "        out.append((df, spec))\n",
        "    return out\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Feature engineering (FAST, no fragmentation)\n",
        "# -----------------------------\n",
        "def make_supervised_lagged(df: pd.DataFrame, spec: DatasetSpec, lags: int, horizon: int) -> Tuple[pd.DataFrame, pd.Series, List[str]]:\n",
        "    df = df.copy()\n",
        "    if spec.time_col is not None:\n",
        "        df = df.sort_values(spec.time_col).reset_index(drop=True)\n",
        "\n",
        "    if spec.time_col is not None:\n",
        "        t = pd.to_datetime(df[spec.time_col])\n",
        "        base = df[spec.feature_cols].copy()\n",
        "        base[\"hour\"] = t.dt.hour\n",
        "        base[\"dayofweek\"] = t.dt.dayofweek\n",
        "        base[\"month\"] = t.dt.month\n",
        "    else:\n",
        "        base = df[spec.feature_cols].copy()\n",
        "\n",
        "    lagged_blocks = []\n",
        "    colnames = []\n",
        "    for L in range(1, lags + 1):\n",
        "        shifted = base.shift(L)\n",
        "        shifted.columns = [f\"{c}_lag{L}\" for c in shifted.columns]\n",
        "        lagged_blocks.append(shifted)\n",
        "        colnames.extend(list(shifted.columns))\n",
        "\n",
        "    X = pd.concat(lagged_blocks, axis=1)\n",
        "    y = df[spec.target_col].shift(-horizon)\n",
        "\n",
        "    valid = X.notnull().all(axis=1) & y.notnull()\n",
        "    X = X.loc[valid].reset_index(drop=True)\n",
        "    y = y.loc[valid].reset_index(drop=True)\n",
        "\n",
        "    return X, y, colnames\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Models + SHAP\n",
        "# -----------------------------\n",
        "def train_tree_model(X_train: np.ndarray, y_train: np.ndarray, seed: int):\n",
        "    model = HistGradientBoostingRegressor(\n",
        "        max_depth=6,\n",
        "        learning_rate=0.08,\n",
        "        max_iter=300,\n",
        "        random_state=seed\n",
        "    )\n",
        "    model.fit(X_train, y_train)\n",
        "    return model\n",
        "\n",
        "def shap_summary_vector(model, X_bg: np.ndarray, X_eval: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Robust SHAP summary for tree ensembles in Colab:\n",
        "      - feature_perturbation='interventional' avoids leaf coverage error\n",
        "      - disable additivity check at shap_values() call\n",
        "    \"\"\"\n",
        "    explainer = shap.TreeExplainer(model, data=X_bg, feature_perturbation=\"interventional\")\n",
        "    shap_vals = explainer.shap_values(X_eval, check_additivity=False)\n",
        "    shap_vals = np.asarray(shap_vals)\n",
        "    return np.mean(np.abs(shap_vals), axis=0)\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# HXAI mechanisms\n",
        "# -----------------------------\n",
        "def split_into_households_contiguous(X: np.ndarray, y: np.ndarray, k: int):\n",
        "    n = len(X)\n",
        "    block = n // k\n",
        "    chunks = []\n",
        "    for i in range(k):\n",
        "        a = i * block\n",
        "        b = (i + 1) * block if i < k - 1 else n\n",
        "        chunks.append((X[a:b], y[a:b]))\n",
        "    return chunks\n",
        "\n",
        "def hxai_zonal_aggregate(local_summaries: List[np.ndarray], epsilons: List[float], sensitivity_l1: float) -> np.ndarray:\n",
        "    d = local_summaries[0].shape[0]\n",
        "    zonal = np.zeros(d, dtype=float)\n",
        "    for Si, eps in zip(local_summaries, epsilons):\n",
        "        scale = sensitivity_l1 / max(eps, 1e-12)\n",
        "        zonal += Si + laplace_noise(scale=scale, size=d)\n",
        "    return zonal\n",
        "\n",
        "def negotiate_epsilons(local_summaries: List[np.ndarray], epsilon_total: float, eps_min: float, eps_max: float, temperature: float = 0.6):\n",
        "    norms = np.array([np.linalg.norm(Si) for Si in local_summaries], dtype=float)\n",
        "    norms = np.maximum(norms, 1e-12)\n",
        "    w = (norms ** temperature)\n",
        "    w = w / np.sum(w)\n",
        "\n",
        "    eps = epsilon_total * w\n",
        "    eps = np.clip(eps, eps_min, eps_max)\n",
        "\n",
        "    # renormalize with clamps\n",
        "    for _ in range(10):\n",
        "        s = float(np.sum(eps))\n",
        "        if s <= 1e-12:\n",
        "            eps[:] = epsilon_total / len(eps)\n",
        "            break\n",
        "        eps *= (epsilon_total / s)\n",
        "        eps = np.clip(eps, eps_min, eps_max)\n",
        "\n",
        "    return eps.tolist()\n",
        "\n",
        "def central_dp_baseline(X: np.ndarray, y: np.ndarray, sensitivity_l1: float, epsilon: float, seed: int):\n",
        "    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
        "    model = train_tree_model(Xtr, ytr, seed=seed)\n",
        "    preds = model.predict(Xte)\n",
        "    metrics = {\"mse\": float(mean_squared_error(yte, preds)), \"r2\": float(r2_score(yte, preds))}\n",
        "\n",
        "    bg = Xtr[np.linspace(0, len(Xtr) - 1, num=min(512, len(Xtr))).astype(int)]\n",
        "    ev = Xte[np.linspace(0, len(Xte) - 1, num=min(1024, len(Xte))).astype(int)]\n",
        "\n",
        "    S = shap_summary_vector(model, bg, ev)\n",
        "    scale = sensitivity_l1 / max(epsilon, 1e-12)\n",
        "    return S + laplace_noise(scale=scale, size=S.shape[0]), metrics\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# One run (one dataset, one seed)\n",
        "# -----------------------------\n",
        "def run_one_seed(\n",
        "    df: pd.DataFrame,\n",
        "    spec: DatasetSpec,\n",
        "    lags: int,\n",
        "    horizon: int,\n",
        "    k_households: int,\n",
        "    eps_grid: List[float],\n",
        "    sensitivity_l1: float,\n",
        "    seed: int,\n",
        ") -> Tuple[pd.DataFrame, Dict]:\n",
        "    set_seed(seed)\n",
        "\n",
        "    Xdf, y, feat_cols = make_supervised_lagged(df, spec, lags=lags, horizon=horizon)\n",
        "    X = Xdf.values.astype(float)\n",
        "    yv = y.values.astype(float)\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X = scaler.fit_transform(X)\n",
        "\n",
        "    chunks = split_into_households_contiguous(X, yv, k=k_households)\n",
        "\n",
        "    local_summaries = []\n",
        "    local_models = []\n",
        "    local_bg_eval = []\n",
        "    local_pred_metrics = []\n",
        "\n",
        "    for i, (Xi, yi) in enumerate(chunks):\n",
        "        Xtr, Xte, ytr, yte = train_test_split(Xi, yi, test_size=0.2, shuffle=False)\n",
        "        model = train_tree_model(Xtr, ytr, seed=seed + i)\n",
        "\n",
        "        preds = model.predict(Xte)\n",
        "        local_pred_metrics.append((float(mean_squared_error(yte, preds)), float(r2_score(yte, preds))))\n",
        "\n",
        "        bg = Xtr[np.linspace(0, len(Xtr) - 1, num=min(256, len(Xtr))).astype(int)]\n",
        "        ev = Xte[np.linspace(0, len(Xte) - 1, num=min(512, len(Xte))).astype(int)]\n",
        "\n",
        "        Si = shap_summary_vector(model, bg, ev)\n",
        "\n",
        "        local_summaries.append(Si)\n",
        "        local_models.append(model)\n",
        "        local_bg_eval.append((bg, ev))\n",
        "\n",
        "    # oracle zonal (no DP)\n",
        "    S_true = np.sum(np.stack(local_summaries, axis=0), axis=0)\n",
        "\n",
        "    rows = []\n",
        "    d = X.shape[1]\n",
        "\n",
        "    for eps in eps_grid:\n",
        "        # HXAI uniform eps\n",
        "        S_same = hxai_zonal_aggregate(local_summaries, [eps] * k_households, sensitivity_l1)\n",
        "\n",
        "        # HXAI negotiated eps (same total budget)\n",
        "        eps_total = k_households * eps\n",
        "        eps_list = negotiate_epsilons(\n",
        "            local_summaries=local_summaries,\n",
        "            epsilon_total=eps_total,\n",
        "            eps_min=max(0.05, 0.10 * eps),\n",
        "            eps_max=10.0 * eps,\n",
        "            temperature=0.6\n",
        "        )\n",
        "        S_neg = hxai_zonal_aggregate(local_summaries, eps_list, sensitivity_l1)\n",
        "\n",
        "        # Central DP baseline\n",
        "        S_central, central_metrics = central_dp_baseline(X, yv, sensitivity_l1, eps, seed=seed)\n",
        "\n",
        "        for method, S_hat in [\n",
        "            (\"Central_DP_SHAP\", S_central),\n",
        "            (\"HXAI_negotiated_eps\", S_neg),\n",
        "            (\"HXAI_same_eps\", S_same),\n",
        "        ]:\n",
        "            rows.append({\n",
        "                \"dataset\": spec.name,\n",
        "                \"seed\": seed,\n",
        "                \"method\": method,\n",
        "                \"epsilon\": eps,\n",
        "                \"utility_cosine\": cosine_similarity(S_true, S_hat),\n",
        "                \"rank_spearman\": spearman_rank_corr(S_true, S_hat),\n",
        "                \"mean_abs_error\": float(np.mean(np.abs(S_true - S_hat))),\n",
        "                \"d_features\": d,\n",
        "                \"k_households\": k_households,\n",
        "                \"sensitivity_l1\": sensitivity_l1,\n",
        "                \"local_avg_mse\": float(np.mean([m for m, _ in local_pred_metrics])),\n",
        "                \"local_avg_r2\": float(np.mean([r for _, r in local_pred_metrics])),\n",
        "                \"central_mse\": central_metrics.get(\"mse\", np.nan),\n",
        "                \"central_r2\": central_metrics.get(\"r2\", np.nan),\n",
        "            })\n",
        "\n",
        "    artifact = {\n",
        "        \"feat_cols\": feat_cols,\n",
        "        \"S_true\": S_true,\n",
        "        \"local_summaries\": local_summaries,\n",
        "        \"k_households\": k_households,\n",
        "        \"d\": d\n",
        "    }\n",
        "    return pd.DataFrame(rows), artifact\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Plot curves with confidence bands (mean ± std across seeds)\n",
        "# -----------------------------\n",
        "def plot_curves_with_bands(res_agg: pd.DataFrame, out_dir: str, dataset_name: str):\n",
        "    ensure_dir(out_dir)\n",
        "\n",
        "    def _plot(metric: str, ylabel: str, fname: str):\n",
        "        fig, ax = plt.subplots(figsize=(6.2, 4.2))\n",
        "        for method in sorted(res_agg[\"method\"].unique()):\n",
        "            rr = res_agg[(res_agg[\"method\"] == method) & (res_agg[\"dataset\"] == dataset_name)].sort_values(\"epsilon\")\n",
        "            x = rr[\"epsilon\"].values\n",
        "            y = rr[f\"{metric}_mean\"].values\n",
        "            s = rr[f\"{metric}_std\"].values\n",
        "            ax.plot(x, y, marker=\"o\", linewidth=2, label=method)\n",
        "            ax.fill_between(x, y - s, y + s, alpha=0.18)\n",
        "\n",
        "        ax.set_xscale(\"log\")\n",
        "        ax.set_xlabel(r\"$\\epsilon$ (log scale)\")\n",
        "        ax.set_ylabel(ylabel)\n",
        "        ax.set_title(f\"{dataset_name}: {ylabel} vs $\\\\epsilon$\")\n",
        "        ieee_axes(ax)\n",
        "        ax.legend(frameon=True)\n",
        "        save_fig(fig, os.path.join(out_dir, f\"curves_{dataset_name}_{fname}\"))\n",
        "\n",
        "    _plot(\"utility_cosine\", \"Utility (cosine similarity)\", \"utility\")\n",
        "    _plot(\"rank_spearman\", \"Ranking stability (Spearman)\", \"rank\")\n",
        "    _plot(\"mean_abs_error\", \"Mean absolute error\", \"mae\")\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Qualitative plots:\n",
        "#   - local vs zonal SHAP bars (noiseless vs noisy)\n",
        "#   - rank trajectories vs epsilon for top-k oracle features\n",
        "# -----------------------------\n",
        "def plot_local_zonal_bars(\n",
        "    dataset_name: str,\n",
        "    feat_cols: List[str],\n",
        "    S_true: np.ndarray,\n",
        "    local_summaries: List[np.ndarray],\n",
        "    eps: float,\n",
        "    sensitivity_l1: float,\n",
        "    out_dir: str,\n",
        "    top_k: int = 12,\n",
        "    household_idx: int = 0,\n",
        "):\n",
        "    ensure_dir(out_dir)\n",
        "\n",
        "    k_households = len(local_summaries)\n",
        "    d = S_true.shape[0]\n",
        "    idx = topk_indices(S_true, top_k)\n",
        "\n",
        "    # oracle local (no noise)\n",
        "    S_local = local_summaries[household_idx].copy()\n",
        "\n",
        "    # local noisy\n",
        "    scale_local = sensitivity_l1 / max(eps, 1e-12)\n",
        "    S_local_noisy = S_local + laplace_noise(scale_local, d)\n",
        "\n",
        "    # zonal noisy (uniform eps)\n",
        "    S_zonal_noisy = hxai_zonal_aggregate(local_summaries, [eps] * k_households, sensitivity_l1)\n",
        "\n",
        "    labels = [feat_cols[i] for i in idx]\n",
        "\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(12.6, 4.4), sharey=True)\n",
        "    titles = [\n",
        "        f\"Local (oracle), h={household_idx}\",\n",
        "        f\"Local (DP), h={household_idx}, eps={eps:g}\",\n",
        "        f\"Zonal (DP), eps={eps:g}\"\n",
        "    ]\n",
        "    vecs = [S_local, S_local_noisy, S_zonal_noisy]\n",
        "\n",
        "    for ax, title, vec in zip(axes, titles, vecs):\n",
        "        vals = vec[idx]\n",
        "        ax.barh(np.arange(len(idx)), vals)\n",
        "        ax.set_yticks(np.arange(len(idx)))\n",
        "        ax.set_yticklabels(labels, fontsize=8)\n",
        "        ax.invert_yaxis()\n",
        "        ax.set_title(title)\n",
        "        ieee_axes(ax)\n",
        "\n",
        "    fig.suptitle(f\"{dataset_name}: Local vs Zonal explanation (Top-{top_k})\", y=1.02)\n",
        "    save_fig(fig, os.path.join(out_dir, f\"qual_{dataset_name}_bars_eps{eps:g}\"))\n",
        "\n",
        "\n",
        "def compute_rank_trajectory(S_true: np.ndarray, S_hat_list: List[np.ndarray]) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Return trajectory ranks for each feature across eps:\n",
        "      rank[t, j] = rank of feature j at eps[t] under S_hat[t], based on abs magnitude.\n",
        "    \"\"\"\n",
        "    T = len(S_hat_list)\n",
        "    d = S_true.shape[0]\n",
        "    ranks = np.zeros((T, d), dtype=int)\n",
        "    for t in range(T):\n",
        "        ranks[t] = rankdata_desc(S_hat_list[t])\n",
        "    return ranks  # ranks are 1..d\n",
        "\n",
        "\n",
        "def plot_rank_trajectories(\n",
        "    dataset_name: str,\n",
        "    feat_cols: List[str],\n",
        "    S_true: np.ndarray,\n",
        "    eps_grid: List[float],\n",
        "    S_hat_by_eps: Dict[str, List[np.ndarray]],\n",
        "    out_dir: str,\n",
        "    top_k: int = 10\n",
        "):\n",
        "    ensure_dir(out_dir)\n",
        "    idx = topk_indices(S_true, top_k)\n",
        "    top_labels = [feat_cols[i] for i in idx]\n",
        "\n",
        "    # oracle ranks for reference\n",
        "    oracle_ranks = rankdata_desc(S_true)\n",
        "    oracle_top_ranks = oracle_ranks[idx]\n",
        "\n",
        "    for method, S_list in S_hat_by_eps.items():\n",
        "        ranks = compute_rank_trajectory(S_true, S_list)  # (T, d)\n",
        "        fig, ax = plt.subplots(figsize=(6.6, 4.4))\n",
        "\n",
        "        for f_i, feat_index in enumerate(idx):\n",
        "            y = ranks[:, feat_index]\n",
        "            ax.plot(eps_grid, y, marker=\"o\", linewidth=1.8, label=f\"{top_labels[f_i]} (oracle rank={oracle_top_ranks[f_i]})\")\n",
        "\n",
        "        ax.set_xscale(\"log\")\n",
        "        ax.set_xlabel(r\"$\\epsilon$ (log scale)\")\n",
        "        ax.set_ylabel(\"Rank (lower is better)\")\n",
        "        ax.set_title(f\"{dataset_name}: Rank trajectories (Top-{top_k}) — {method}\")\n",
        "        ax.invert_yaxis()\n",
        "        ieee_axes(ax)\n",
        "\n",
        "        # Keep legend readable: place outside for many lines\n",
        "        ax.legend(loc=\"center left\", bbox_to_anchor=(1.02, 0.5), fontsize=8, frameon=True)\n",
        "        save_fig(fig, os.path.join(out_dir, f\"qual_{dataset_name}_ranktraj_{method}\"))\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Build noisy S_hat lists for qualitative rank plots (one seed, one dataset)\n",
        "# -----------------------------\n",
        "def build_noisy_explanations_over_eps(\n",
        "    S_true: np.ndarray,\n",
        "    local_summaries: List[np.ndarray],\n",
        "    X_all: np.ndarray,\n",
        "    y_all: np.ndarray,\n",
        "    eps_grid: List[float],\n",
        "    sensitivity_l1: float,\n",
        "    seed: int\n",
        ") -> Dict[str, List[np.ndarray]]:\n",
        "    set_seed(seed)\n",
        "\n",
        "    k_households = len(local_summaries)\n",
        "\n",
        "    S_same_list = []\n",
        "    S_neg_list = []\n",
        "    S_cent_list = []\n",
        "\n",
        "    for eps in eps_grid:\n",
        "        S_same = hxai_zonal_aggregate(local_summaries, [eps] * k_households, sensitivity_l1)\n",
        "\n",
        "        eps_total = k_households * eps\n",
        "        eps_list = negotiate_epsilons(\n",
        "            local_summaries=local_summaries,\n",
        "            epsilon_total=eps_total,\n",
        "            eps_min=max(0.05, 0.10 * eps),\n",
        "            eps_max=10.0 * eps,\n",
        "            temperature=0.6\n",
        "        )\n",
        "        S_neg = hxai_zonal_aggregate(local_summaries, eps_list, sensitivity_l1)\n",
        "\n",
        "        S_cent, _ = central_dp_baseline(X_all, y_all, sensitivity_l1, eps, seed=seed)\n",
        "\n",
        "        S_same_list.append(S_same)\n",
        "        S_neg_list.append(S_neg)\n",
        "        S_cent_list.append(S_cent)\n",
        "\n",
        "    return {\n",
        "        \"HXAI_same_eps\": S_same_list,\n",
        "        \"HXAI_negotiated_eps\": S_neg_list,\n",
        "        \"Central_DP_SHAP\": S_cent_list\n",
        "    }\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Main\n",
        "# -----------------------------\n",
        "def main(args):\n",
        "    ensure_dir(args.out_dir)\n",
        "    ensure_dir(args.data_dir)\n",
        "\n",
        "    # eps grid (log-spaced)\n",
        "    eps_grid = np.logspace(math.log10(args.eps_min), math.log10(args.eps_max), args.eps_points).tolist()\n",
        "\n",
        "    # seeds list\n",
        "    seeds = [int(s) for s in args.seeds.split(\",\") if s.strip()]\n",
        "\n",
        "    # datasets\n",
        "    datasets = get_all_datasets(args.data_dir, args.datasets)\n",
        "\n",
        "    all_rows = []\n",
        "\n",
        "    # For qualitative plots, store one \"representative seed artifact\" per dataset (first seed)\n",
        "    qual_artifacts = {}\n",
        "\n",
        "    for (df, spec) in datasets:\n",
        "        print(f\"\\n[dataset] {spec.name}  n={len(df)}  d_raw={len(spec.feature_cols)}  time={spec.time_col}\")\n",
        "        for si, seed in enumerate(seeds):\n",
        "            res_seed, artifact = run_one_seed(\n",
        "                df=df, spec=spec,\n",
        "                lags=args.lags, horizon=args.horizon,\n",
        "                k_households=args.k_households,\n",
        "                eps_grid=eps_grid,\n",
        "                sensitivity_l1=args.sensitivity_l1,\n",
        "                seed=seed\n",
        "            )\n",
        "            all_rows.append(res_seed)\n",
        "\n",
        "            if si == 0:\n",
        "                # For qual plots we also need X_all / y_all aligned with lags\n",
        "                Xdf, y, feat_cols = make_supervised_lagged(df, spec, args.lags, args.horizon)\n",
        "                X_all = StandardScaler().fit_transform(Xdf.values.astype(float))\n",
        "                y_all = y.values.astype(float)\n",
        "\n",
        "                qual_artifacts[spec.name] = {\n",
        "                    \"seed\": seed,\n",
        "                    \"feat_cols\": artifact[\"feat_cols\"],\n",
        "                    \"S_true\": artifact[\"S_true\"],\n",
        "                    \"local_summaries\": artifact[\"local_summaries\"],\n",
        "                    \"eps_grid\": eps_grid,\n",
        "                    \"X_all\": X_all,\n",
        "                    \"y_all\": y_all\n",
        "                }\n",
        "\n",
        "    res_all = pd.concat(all_rows, axis=0).reset_index(drop=True)\n",
        "    per_seed_csv = os.path.join(args.out_dir, \"results_per_seed.csv\")\n",
        "    res_all.to_csv(per_seed_csv, index=False)\n",
        "    print(f\"\\n[saved] {per_seed_csv}\")\n",
        "\n",
        "    # Aggregate mean/std across seeds\n",
        "    agg = (\n",
        "        res_all\n",
        "        .groupby([\"dataset\", \"method\", \"epsilon\"], as_index=False)\n",
        "        .agg(\n",
        "            utility_cosine_mean=(\"utility_cosine\", \"mean\"),\n",
        "            utility_cosine_std=(\"utility_cosine\", \"std\"),\n",
        "            rank_spearman_mean=(\"rank_spearman\", \"mean\"),\n",
        "            rank_spearman_std=(\"rank_spearman\", \"std\"),\n",
        "            mean_abs_error_mean=(\"mean_abs_error\", \"mean\"),\n",
        "            mean_abs_error_std=(\"mean_abs_error\", \"std\"),\n",
        "            d_features=(\"d_features\", \"first\"),\n",
        "            k_households=(\"k_households\", \"first\"),\n",
        "            sensitivity_l1=(\"sensitivity_l1\", \"first\")\n",
        "        )\n",
        "    )\n",
        "    agg_csv = os.path.join(args.out_dir, \"results_agg_meanstd.csv\")\n",
        "    agg.to_csv(agg_csv, index=False)\n",
        "    print(f\"[saved] {agg_csv}\")\n",
        "\n",
        "    # Curves with bands\n",
        "    for (df, spec) in datasets:\n",
        "        plot_curves_with_bands(agg, args.out_dir, spec.name)\n",
        "\n",
        "    # Qualitative plots (bars + rank trajectories) for each dataset using representative seed\n",
        "    for ds_name, art in qual_artifacts.items():\n",
        "        feat_cols = art[\"feat_cols\"]\n",
        "        S_true = art[\"S_true\"]\n",
        "        local_summaries = art[\"local_summaries\"]\n",
        "        eps_grid = art[\"eps_grid\"]\n",
        "        X_all = art[\"X_all\"]\n",
        "        y_all = art[\"y_all\"]\n",
        "        seed = art[\"seed\"]\n",
        "\n",
        "        # choose eps values to visualize (closest in grid)\n",
        "        def closest_eps(target: float):\n",
        "            arr = np.array(eps_grid)\n",
        "            return float(arr[np.argmin(np.abs(arr - target))])\n",
        "\n",
        "        eps_low = closest_eps(args.qual_eps_low)\n",
        "        eps_mid = closest_eps(args.qual_eps_mid)\n",
        "\n",
        "        plot_local_zonal_bars(\n",
        "            dataset_name=ds_name,\n",
        "            feat_cols=feat_cols,\n",
        "            S_true=S_true,\n",
        "            local_summaries=local_summaries,\n",
        "            eps=eps_low,\n",
        "            sensitivity_l1=args.sensitivity_l1,\n",
        "            out_dir=args.out_dir,\n",
        "            top_k=args.qual_top_k,\n",
        "            household_idx=min(args.qual_household_idx, len(local_summaries) - 1)\n",
        "        )\n",
        "        plot_local_zonal_bars(\n",
        "            dataset_name=ds_name,\n",
        "            feat_cols=feat_cols,\n",
        "            S_true=S_true,\n",
        "            local_summaries=local_summaries,\n",
        "            eps=eps_mid,\n",
        "            sensitivity_l1=args.sensitivity_l1,\n",
        "            out_dir=args.out_dir,\n",
        "            top_k=args.qual_top_k,\n",
        "            household_idx=min(args.qual_household_idx, len(local_summaries) - 1)\n",
        "        )\n",
        "\n",
        "        # Rank trajectories across eps for each method\n",
        "        S_hat_by_eps = build_noisy_explanations_over_eps(\n",
        "            S_true=S_true,\n",
        "            local_summaries=local_summaries,\n",
        "            X_all=X_all,\n",
        "            y_all=y_all,\n",
        "            eps_grid=eps_grid,\n",
        "            sensitivity_l1=args.sensitivity_l1,\n",
        "            seed=seed\n",
        "        )\n",
        "        plot_rank_trajectories(\n",
        "            dataset_name=ds_name,\n",
        "            feat_cols=feat_cols,\n",
        "            S_true=S_true,\n",
        "            eps_grid=eps_grid,\n",
        "            S_hat_by_eps=S_hat_by_eps,\n",
        "            out_dir=args.out_dir,\n",
        "            top_k=args.rank_top_k\n",
        "        )\n",
        "\n",
        "    # Quick textual summary\n",
        "    print(\"\\nQuick check (best utility per dataset/method):\")\n",
        "    best = (\n",
        "        agg.sort_values(\"utility_cosine_mean\", ascending=False)\n",
        "           .groupby([\"dataset\", \"method\"], as_index=False)\n",
        "           .head(1)[[\"dataset\", \"method\", \"epsilon\", \"utility_cosine_mean\", \"rank_spearman_mean\", \"mean_abs_error_mean\"]]\n",
        "    )\n",
        "    print(best.to_string(index=False))\n",
        "\n",
        "    print(\"\\nDone. Check outputs in:\", args.out_dir)\n",
        "\n",
        "\n",
        "def build_argparser():\n",
        "    ap = argparse.ArgumentParser()\n",
        "    ap.add_argument(\"--out_dir\", type=str, default=\"hxai_realdata_out\")\n",
        "    ap.add_argument(\"--data_dir\", type=str, default=\"hxai_realdata_data\")\n",
        "\n",
        "    ap.add_argument(\"--datasets\", type=str, default=\"all\",\n",
        "                    help=\"all OR comma-separated: uci_appliances,uci_household_power,uci_bikesharing_hourly\")\n",
        "\n",
        "    ap.add_argument(\"--k_households\", type=int, default=20)\n",
        "    ap.add_argument(\"--lags\", type=int, default=6)\n",
        "    ap.add_argument(\"--horizon\", type=int, default=1)\n",
        "\n",
        "    ap.add_argument(\"--eps_min\", type=float, default=0.1)\n",
        "    ap.add_argument(\"--eps_max\", type=float, default=20.0)\n",
        "    ap.add_argument(\"--eps_points\", type=int, default=10)\n",
        "\n",
        "    ap.add_argument(\"--sensitivity_l1\", type=float, default=1.0)\n",
        "\n",
        "    ap.add_argument(\"--seeds\", type=str, default=\"42,43,44\",\n",
        "                    help=\"comma-separated seeds for mean±std bands\")\n",
        "\n",
        "    # qualitative plots\n",
        "    ap.add_argument(\"--qual_eps_low\", type=float, default=0.2)\n",
        "    ap.add_argument(\"--qual_eps_mid\", type=float, default=1.0)\n",
        "    ap.add_argument(\"--qual_top_k\", type=int, default=12)\n",
        "    ap.add_argument(\"--qual_household_idx\", type=int, default=0)\n",
        "\n",
        "    ap.add_argument(\"--rank_top_k\", type=int, default=10)\n",
        "\n",
        "    return ap\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = build_argparser()\n",
        "    # Jupyter/Colab safe\n",
        "    args, _unknown = parser.parse_known_args()\n",
        "    main(args)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}